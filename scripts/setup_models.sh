#!/bin/bash

# Setup script for converting PyTorch models to ONNX format
# This script automates the conversion process for both Fin-O models

set -e  # Exit on any error

echo "ðŸš€ Setting up Fin-O models for browser inference..."

# Check if Python is available
if ! command -v python3 &> /dev/null; then
    echo "âŒ Python 3 is required but not installed."
    exit 1
fi

# Check if PyTorch is installed
python3 -c "import torch" 2>/dev/null || {
    echo "âŒ PyTorch is required but not installed."
    echo "Install it with: pip install torch"
    exit 1
}

# Create necessary directories
echo "ðŸ“ Creating directories..."
mkdir -p public/model
mkdir -p scripts

# Check if model files exist
if [ ! -f "model/fin-o-small" ]; then
    echo "âš ï¸  Warning: model/fin-o-small not found"
fi

if [ ! -f "model/fin-o-large" ]; then
    echo "âš ï¸  Warning: model/fin-o-large not found"
fi

if [ ! -f "model/vocab.json" ]; then
    echo "âŒ Error: model/vocab.json not found"
    exit 1
fi

# Convert Fin-O Small model
echo "ðŸ”„ Converting Fin-O Small model..."
if [ -f "model/fin-o-small" ]; then
    python3 scripts/convert_model_to_onnx.py \
        --model-type small \
        --model-path model/fin-o-small \
        --output-dir public/model
else
    echo "âš ï¸  Skipping Fin-O Small conversion (file not found)"
fi

# Convert Fin-O Large model
echo "ðŸ”„ Converting Fin-O Large model..."
if [ -f "model/fin-o-large" ]; then
    python3 scripts/convert_model_to_onnx.py \
        --model-type large \
        --model-path model/fin-o-large \
        --output-dir public/model
else
    echo "âš ï¸  Skipping Fin-O Large conversion (file not found)"
fi

# Copy vocab.json to public directory
echo "ðŸ“‹ Copying vocabulary file..."
cp model/vocab.json public/model/vocab.json

# Create a simple scaler.json if it doesn't exist
if [ ! -f "public/model/scaler.json" ]; then
    echo "ðŸ“Š Creating default scaler configuration..."
    cat > public/model/scaler.json << 'EOF'
{
  "scale_": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
  "mean_": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
  "var_": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],
  "n_samples_seen_": 1000
}
EOF
fi

echo "âœ… Model setup complete!"
echo ""
echo "ðŸ“ Files created in public/model/:"
ls -la public/model/
echo ""
echo "ðŸŒ You can now access the model inference at:"
echo "   http://localhost:3000/model-inference"
echo "   http://localhost:3000/test-model"
echo ""
echo "ðŸ“š For more information, see the README.md file."
