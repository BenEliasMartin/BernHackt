{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "3VCCEYZpw-ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install google-cloud-storage"
      ],
      "metadata": {
        "id": "J9C_teZlx_UK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "5fcb9f62-0033-426b-eaac-99389b75af5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (2.32.4)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage) (1.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2025.8.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.cloud import storage\n",
        "import os\n",
        "import subprocess"
      ],
      "metadata": {
        "id": "3zBztIRnyIkG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "ArG7Rmppw6YL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "Dv6nl9EGtO0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   **generated dataset**\n",
        "\n",
        "- step\n",
        "- action\n",
        "- amount\n",
        "- nameOrig\n",
        "- oldBalanceOrig\n",
        "- newBalanceOrig [DROP]\n",
        "- nameDest\n",
        "- oldBalanceDest [DROP]\n",
        "- newBalanceDest [DROP]\n",
        "- isFraud [DROP]\n",
        "- isFlaggedFraud [DROP]\n",
        "- isUnauthorizedOverdraft [DROP]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2.   **https://github.com/spendcastai/bernhackt-2025**\n",
        "\n",
        "- transaction_id\n",
        "- transaction_name\n",
        "- amount\n",
        "- date\n",
        "- type\n",
        "- status\n",
        "- card_number\n",
        "- category\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Maybe consider as well https://www.kaggle.com/datasets/priyamchoksi/credit-card-transactions-dataset"
      ],
      "metadata": {
        "id": "KkYPRYl9tUAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "aaM5OgAqxLrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 50\n",
        "FORECAST_HORIZON = 10\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "INPUT_FEATURES = 14  # 2 categorical IDs + 12 numerical features\n",
        "OUTPUT_FEATURES = 3 # amount, category_id, merchant_id\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 512\n",
        "NUM_LAYERS = 4\n",
        "DROPOUT_PROB = 0.2\n",
        "LEARNING_RATE = 0.003\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "a9TDCu0Awdyf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Data-loading"
      ],
      "metadata": {
        "id": "kifxVyHU03jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_raw_data(df: pd.DataFrame):\n",
        "    is_income = df['action'].str.contains(\"INCOME\", case=False)\n",
        "\n",
        "    df['amount'] = df['amount'] * is_income.replace({True: 1, False: -1})\n",
        "    df['user_id'], user_vocab = pd.factorize(df['nameOrig'])\n",
        "    df['category_id'], cat_vocab = pd.factorize(df['action'])\n",
        "    df['merchant_id'], merch_vocab = pd.factorize(df['nameDest'])\n",
        "    df = df.rename(columns={\"oldBalanceOrig\": \"balance_before\"})\n",
        "    df = df.sort_values(by=['user_id', 'step'])\n",
        "    df['time_delta'] = df.groupby('user_id')['step'].diff().fillna(0)\n",
        "    df['time_delta_category'] = df.groupby(['user_id', 'category_id'])['step'].diff().fillna(0)\n",
        "    df['time_delta_merchant'] = df.groupby(['user_id', 'merchant_id'])['step'].diff().fillna(0)\n",
        "    expanding_mean = df.groupby(['user_id', 'merchant_id'])['amount'].expanding().mean()\n",
        "    df['avg_amount_merchant'] = expanding_mean.reset_index(level=[0,1], drop=True)\n",
        "    df['avg_amount_merchant'] = df.groupby(['user_id', 'merchant_id'])['avg_amount_merchant'].shift(1).fillna(0)\n",
        "\n",
        "    df['datetime'] = pd.to_datetime(df['step'], unit='D', origin='2024-01-01')\n",
        "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "    df['month_of_year'] = df['datetime'].dt.month\n",
        "    df['day_of_month'] = df['datetime'].dt.day\n",
        "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_of_month_sin'] = np.sin(2 * np.pi * df['day_of_month'] / 31)\n",
        "    df['day_of_month_cos'] = np.cos(2 * np.pi * df['day_of_month'] / 31)\n",
        "    df['month_of_year_sin'] = np.sin(2 * np.pi * df['month_of_year'] / 12)\n",
        "    df['month_of_year_cos'] = np.cos(2 * np.pi * df['month_of_year'] / 12)\n",
        "\n",
        "    numerical_features = [\n",
        "        'amount', 'balance_before', 'time_delta',\n",
        "        'time_delta_category', 'time_delta_merchant', 'avg_amount_merchant',\n",
        "        'day_of_week_sin', 'day_of_week_cos',\n",
        "        'day_of_month_sin', 'day_of_month_cos',\n",
        "        'month_of_year_sin', 'month_of_year_cos'\n",
        "    ]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "    features_to_keep = [\n",
        "        'user_id', 'category_id', 'merchant_id'\n",
        "    ] + numerical_features\n",
        "\n",
        "    vocab_mappings = {\n",
        "        'categories': list(cat_vocab),\n",
        "        'merchants': list(merch_vocab)\n",
        "    }\n",
        "\n",
        "    return df[features_to_keep], vocab_mappings, scaler"
      ],
      "metadata": {
        "id": "AX9rHOyyyBhW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df: pd.DataFrame, sequence_length: int, forecast_horizon: int):\n",
        "    input_features = [\n",
        "        'amount', 'balance_before', 'category_id', 'merchant_id', 'time_delta',\n",
        "        'time_delta_category', 'time_delta_merchant', 'avg_amount_merchant',\n",
        "        'day_of_week_sin', 'day_of_week_cos',\n",
        "        'day_of_month_sin', 'day_of_month_cos',\n",
        "        'month_of_year_sin', 'month_of_year_cos'\n",
        "    ]\n",
        "\n",
        "    target_features = ['amount', 'category_id', 'merchant_id', 'time_delta']\n",
        "\n",
        "    all_sequences_X = []\n",
        "    all_sequences_y = []\n",
        "\n",
        "    total_users = df['user_id'].nunique()\n",
        "    for i, user_id in enumerate(df['user_id'].unique()):\n",
        "        user_df = df[df['user_id'] == user_id]\n",
        "\n",
        "        if len(user_df) < sequence_length + forecast_horizon:\n",
        "            continue\n",
        "\n",
        "        user_X_data = user_df[input_features].values\n",
        "        user_y_data = user_df[target_features].values\n",
        "\n",
        "        for j in range(len(user_df) - sequence_length - forecast_horizon + 1):\n",
        "            start_idx = j\n",
        "            mid_idx = j + sequence_length\n",
        "            end_idx = mid_idx + forecast_horizon\n",
        "\n",
        "            all_sequences_X.append(user_X_data[start_idx:mid_idx])\n",
        "            all_sequences_y.append(user_y_data[mid_idx:end_idx])\n",
        "\n",
        "    return all_sequences_X, all_sequences_y"
      ],
      "metadata": {
        "id": "oEkcnQ0GytM9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = 'bernhackt'\n",
        "source_data_path = f'gs://{bucket_name}/t2.csv'\n",
        "local_source_path = Path('t2.csv')\n",
        "processed_data_path = f'gs://{bucket_name}/processed_data.zip'\n",
        "local_zip_path = Path('processed_data.zip')\n",
        "local_data_dir = Path('processed_data')\n",
        "\n",
        "sequences_X_path = local_data_dir / 'sequences_X.npy'\n",
        "sequences_y_path = local_data_dir / 'sequences_y.npy'\n",
        "vocab_path = local_data_dir / 'vocab.json'\n",
        "scaler_path = local_data_dir / 'scaler.pkl'\n",
        "\n",
        "gcs_check_command = f\"gcloud storage ls {processed_data_path}\"\n",
        "result = subprocess.run(gcs_check_command, shell=True, capture_output=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    # Data exists\n",
        "    print(f\"✅ Found 'processed_data.zip' in GCS bucket '{bucket_name}'.\")\n",
        "    print(\"Downloading to the Colab environment...\")\n",
        "    !gcloud storage cp {processed_data_path} {local_zip_path}\n",
        "\n",
        "    print(\"Unzipping data...\")\n",
        "    !unzip -qo {local_zip_path}\n",
        "    print(\"Data is ready.\")\n",
        "\n",
        "else:\n",
        "    # Data does not exists\n",
        "    print(f\"❌ 'processed_data.zip' not found in GCS bucket '{bucket_name}'.\")\n",
        "    print(\"Processing raw data from scratch...\")\n",
        "\n",
        "    !gcloud storage cp {source_data_path} {local_source_path}\n",
        "\n",
        "    raw_df = pd.read_csv(local_source_path)\n",
        "    processed_df, vocab_mappings, scaler = process_raw_data(raw_df)\n",
        "\n",
        "    print(\"Creating sequences...\")\n",
        "    sequences_X_list, sequences_y_list = create_sequences(processed_df, SEQUENCE_LENGTH, FORECAST_HORIZON)\n",
        "\n",
        "    print(\"Converting sequences to contiguous NumPy arrays. This may take a moment...\")\n",
        "    sequences_X = np.array(sequences_X_list, dtype=np.float32)\n",
        "    sequences_y = np.array(sequences_y_list, dtype=np.float32)\n",
        "\n",
        "    del sequences_X_list, sequences_y_list, processed_df, raw_df\n",
        "\n",
        "    local_data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"Saving correctly formatted .npy files to the local Colab environment...\")\n",
        "    np.save(sequences_X_path, sequences_X)\n",
        "    np.save(sequences_y_path, sequences_y)\n",
        "    with open(vocab_path, 'w') as f:\n",
        "        json.dump(vocab_mappings, f, indent=4)\n",
        "    with open(scaler_path, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    del sequences_X, sequences_y\n",
        "\n",
        "    print(f\"Zipping the '{local_data_dir}' directory...\")\n",
        "    !zip -r {local_zip_path} {local_data_dir}\n",
        "\n",
        "    print(f\"Uploading '{local_zip_path}' to GCS for backup...\")\n",
        "    !gcloud storage cp {local_zip_path} {processed_data_path}\n",
        "    print(\"Upload complete. Data is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkuPbO97zMxk",
        "outputId": "90bb2fb2-c469-4cfc-e95c-5e608973c4aa",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Found 'processed_data.zip' in GCS bucket 'bernhackt'.\n",
            "Downloading to the Colab environment...\n",
            "Copying gs://bernhackt/processed_data.zip to file://processed_data.zip\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\u001b[1;33mWARNING:\u001b[0m Component check failed. Could not verify SDK install path.\n",
            "\n",
            "Average throughput: 76.3MiB/s\n",
            "Unzipping data...\n",
            "Data is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(vocab_path, 'r') as f:\n",
        "    vocab_mappings = json.load(f)\n",
        "\n",
        "with open(scaler_path, 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "vocab_sizes = {\n",
        "    'categories': len(vocab_mappings['categories']),\n",
        "    'merchants': len(vocab_mappings['merchants'])\n",
        "}\n",
        "\n",
        "print(\"Vocab mappings and scaler loaded into memory.\")"
      ],
      "metadata": {
        "id": "mESky7D75zAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118ffbc8-69a1-4416-9b70-387fb774c21f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab mappings and scaler loaded into memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, x_path, y_path, vocab_sizes, indices):\n",
        "        self.sequences_X = np.load(x_path, mmap_mode='r')\n",
        "        self.sequences_y = np.load(y_path, mmap_mode='r')\n",
        "\n",
        "        self.indices = indices\n",
        "\n",
        "        self.vocab_size_cat = vocab_sizes['categories']\n",
        "        self.vocab_size_merch = vocab_sizes['merchants']\n",
        "\n",
        "        assert len(self.sequences_X) == len(self.sequences_y), \"X and y sequences must have the same length.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_item = np.copy(self.sequences_X[self.indices[idx]])\n",
        "        y_item = np.copy(self.sequences_y[self.indices[idx]])\n",
        "\n",
        "        category_id_in = x_item[:, 2]\n",
        "        merchant_id_in = x_item[:, 3]\n",
        "\n",
        "        if np.any(category_id_in >= self.vocab_size_cat) or np.any(merchant_id_in >= self.vocab_size_merch):\n",
        "            raise IndexError(f\"Data at index {self.indices[idx]} contains an out-of-bounds category or merchant ID.\")\n",
        "\n",
        "        return torch.from_numpy(x_item).float(), torch.from_numpy(y_item).float()"
      ],
      "metadata": {
        "id": "4enhfnO36PsN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = np.load(sequences_X_path, mmap_mode='r').shape[0]\n",
        "all_indices = np.arange(num_samples)\n",
        "train_indices, val_indices = train_test_split(\n",
        "    all_indices,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Training indices: {len(train_indices)}\")\n",
        "print(f\"Validation indices: {len(val_indices)}\")\n",
        "\n",
        "train_dataset = Dataset(sequences_X_path, sequences_y_path, vocab_sizes, train_indices)\n",
        "val_dataset = Dataset(sequences_X_path, sequences_y_path, vocab_sizes, val_indices)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"\\nNumber of training batches: {len(train_dataloader)}\")\n",
        "print(f\"Number of validation batches: {len(val_dataloader)}\")"
      ],
      "metadata": {
        "id": "XQvDsTyn6_NG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e4dcb6-62bc-4af0-b256-1e393979aab9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training indices: 6020639\n",
            "Validation indices: 1505160\n",
            "\n",
            "Number of training batches: 2940\n",
            "Number of validation batches: 735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-training"
      ],
      "metadata": {
        "id": "aIRq8orT0dEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Architecture (LSTM)\n"
      ],
      "metadata": {
        "id": "z9VdOkhS08yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size_cat, vocab_size_merch, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.category_embedding = nn.Embedding(vocab_size_cat, embedding_dim)\n",
        "        self.merchant_embedding = nn.Embedding(vocab_size_merch, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=12 + (embedding_dim * 2),\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        numerical_feats = x[:, :, [0,1,4,5,6,7,8,9,10,11,12,13]]\n",
        "        cat_ids = x[:, :, 2].long()\n",
        "        merch_ids = x[:, :, 3].long()\n",
        "\n",
        "        cat_embeds = self.category_embedding(cat_ids)\n",
        "        merch_embeds = self.merchant_embedding(merch_ids)\n",
        "\n",
        "        lstm_input = torch.cat((numerical_feats, cat_embeds, merch_embeds), dim=2)\n",
        "\n",
        "        _, (hidden, cell) = self.lstm(lstm_input)\n",
        "        return hidden, cell"
      ],
      "metadata": {
        "id": "xriUNAK405_r"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size_cat, vocab_size_merch, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.vocab_size_cat = vocab_size_cat\n",
        "        self.vocab_size_merch = vocab_size_merch\n",
        "\n",
        "\n",
        "        self.category_embedding = nn.Embedding(vocab_size_cat, embedding_dim)\n",
        "        self.merchant_embedding = nn.Embedding(vocab_size_merch, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=1 + (embedding_dim * 2),\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.fc_amount = nn.Linear(hidden_dim, 1)\n",
        "        self.fc_category = nn.Linear(hidden_dim, vocab_size_cat)\n",
        "        self.fc_merchant = nn.Linear(hidden_dim, vocab_size_merch)\n",
        "\n",
        "    def forward(self, x_t, hidden, cell):\n",
        "        x_t = x_t.unsqueeze(1)\n",
        "\n",
        "        numerical_feat = x_t[:, :, 0].unsqueeze(-1)\n",
        "        cat_ids = x_t[:, :, 1].long()\n",
        "        merch_ids = x_t[:, :, 2].long()\n",
        "\n",
        "        cat_embeds = self.category_embedding(cat_ids)\n",
        "        merch_embeds = self.merchant_embedding(merch_ids)\n",
        "\n",
        "        lstm_input = torch.cat((numerical_feat, cat_embeds, merch_embeds), dim=2)\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
        "\n",
        "        pred_amount = self.fc_amount(output.squeeze(1))\n",
        "        pred_category = self.fc_category(output.squeeze(1))\n",
        "        pred_merchant = self.fc_merchant(output.squeeze(1))\n",
        "\n",
        "        return pred_amount, pred_category, pred_merchant, hidden, cell"
      ],
      "metadata": {
        "id": "IRdhQYevG9AT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        forecast_horizon = trg.shape[1]\n",
        "\n",
        "        outputs_amount = torch.zeros(batch_size, forecast_horizon, 1).to(self.device)\n",
        "        outputs_category = torch.zeros(batch_size, forecast_horizon, self.decoder.vocab_size_cat).to(self.device)\n",
        "        outputs_merchant = torch.zeros(batch_size, forecast_horizon, self.decoder.vocab_size_merch).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        decoder_input = trg[:, 0, :]\n",
        "\n",
        "        for t in range(forecast_horizon):\n",
        "            pred_amount, pred_category, pred_merchant, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "            outputs_amount[:, t, :] = pred_amount\n",
        "            outputs_category[:, t, :] = pred_category\n",
        "            outputs_merchant[:, t, :] = pred_merchant\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            if teacher_force:\n",
        "                decoder_input = trg[:, t, :]\n",
        "            else:\n",
        "                top_category_id = pred_category.argmax(1).long()\n",
        "                top_merchant_id = pred_merchant.argmax(1).long()\n",
        "\n",
        "                pred_cat_embeds = self.decoder.category_embedding(top_category_id)\n",
        "                pred_merch_embeds = self.decoder.merchant_embedding(top_merchant_id)\n",
        "\n",
        "                decoder_input = torch.cat((pred_amount, top_category_id.unsqueeze(1).float(), top_merchant_id.unsqueeze(1).float()), dim=1)\n",
        "\n",
        "\n",
        "        return outputs_amount, outputs_category, outputs_merchant\n",
        "\n",
        "    def predict(self, src, forecast_horizon):\n",
        "        self.eval()\n",
        "        batch_size = src.shape[0]\n",
        "\n",
        "        outputs_amount = torch.zeros(batch_size, forecast_horizon, 1).to(self.device)\n",
        "        outputs_category = torch.zeros(batch_size, forecast_horizon, self.decoder.vocab_size_cat).to(self.device)\n",
        "        outputs_merchant = torch.zeros(batch_size, forecast_horizon, self.decoder.vocab_size_merch).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hidden, cell = self.encoder(src)\n",
        "\n",
        "            last_known_amount = src[:, -1, 0].unsqueeze(1)\n",
        "            last_known_cat_id = src[:, -1, 2]\n",
        "            last_known_merch_id = src[:, -1, 3]\n",
        "            decoder_input = torch.cat([last_known_amount, last_known_cat_id.unsqueeze(1), last_known_merch_id.unsqueeze(1)], dim=1)\n",
        "\n",
        "            for t in range(forecast_horizon):\n",
        "                pred_amount, pred_category, pred_merchant, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "                outputs_amount[:, t, :] = pred_amount\n",
        "                outputs_category[:, t, :] = pred_category\n",
        "                outputs_merchant[:, t, :] = pred_merchant\n",
        "\n",
        "                # Create the input for the next time step from the current prediction\n",
        "                top_category_id = pred_category.argmax(1).float()\n",
        "                top_merchant_id = pred_merchant.argmax(1).float()\n",
        "                decoder_input = torch.cat((pred_amount, top_category_id.unsqueeze(1), top_merchant_id.unsqueeze(1)), dim=1)\n",
        "\n",
        "        return outputs_amount, outputs_category, outputs_merchant"
      ],
      "metadata": {
        "id": "TdCotfUjHYFN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, criterion_h, criterion_ce, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "\n",
        "    for i, (X_batch, y_batch) in enumerate(dataloader):\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_amount, pred_category, pred_merchant = model(X_batch, y_batch)\n",
        "\n",
        "        loss_amount = criterion_h(pred_amount.squeeze(-1), y_batch[:, :, 0])\n",
        "        loss_category = criterion_ce(\n",
        "            pred_category.view(-1, pred_category.shape[-1]),\n",
        "            y_batch[:, :, 1].view(-1).long()\n",
        "        )\n",
        "        loss_merchant = criterion_ce(\n",
        "            pred_merchant.view(-1, pred_merchant.shape[-1]),\n",
        "            y_batch[:, :, 2].view(-1).long()\n",
        "        )\n",
        "\n",
        "        total_loss = loss_amount * 0.0 + loss_category * 0.5 + loss_merchant * 0.4\n",
        "\n",
        "        total_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += total_loss.item()\n",
        "\n",
        "        print(f\"\\rEpoch {epoch+1}/{NUM_EPOCHS} | Batch {int((i+1)/len(dataloader)*100)}% | Loss: {total_loss.item():.4f}\", end=\"\")\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "OT_RKswIHdVa"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model_state_dict, epoch, bucket_name):\n",
        "    local_save_dir = Path('model')\n",
        "    local_save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    local_model_path = local_save_dir / f'model_epoch_{epoch+1}.pth'\n",
        "    torch.save(model_state_dict, local_model_path)\n",
        "    print(f\"Model saved locally to '{local_model_path}'\")\n",
        "\n",
        "    gcs_model_path = f'gs://{bucket_name}/models/model_epoch_{epoch+1}.pth'\n",
        "    print(f\"Uploading model to GCS: {gcs_model_path}\")\n",
        "    !gcloud storage cp {local_model_path} {gcs_model_path}\n",
        "    print(\"Upload complete.\")"
      ],
      "metadata": {
        "id": "hKAhk-br8wEh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(model, dataloader, criterion_h, criterion_ce, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            outputs = model(X_batch, y_batch, teacher_forcing_ratio=0)\n",
        "\n",
        "            amount_pred, category_pred, merchant_pred = outputs\n",
        "            amount_true, category_true, merchant_true = y_batch[:, :, 0], y_batch[:, :, 1].long(), y_batch[:, :, 2].long()\n",
        "\n",
        "            loss_h = criterion_h(amount_pred.squeeze(-1), amount_true)\n",
        "            loss_cat = criterion_ce(category_pred.view(-1, VOCAB_SIZE_CAT), category_true.view(-1))\n",
        "            loss_merch = criterion_ce(merchant_pred.view(-1, VOCAB_SIZE_MERCH), merchant_true.view(-1))\n",
        "\n",
        "            combined_loss = loss_h *0.0 + loss_cat * 0.5 + loss_merch *0.4\n",
        "            total_loss += combined_loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "t9KQ9zoOX73I"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE_CAT = vocab_sizes['categories']\n",
        "VOCAB_SIZE_MERCH = vocab_sizes['merchants']\n",
        "\n",
        "encoder = Encoder(VOCAB_SIZE_CAT, VOCAB_SIZE_MERCH, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_PROB)\n",
        "decoder = Decoder(VOCAB_SIZE_CAT, VOCAB_SIZE_MERCH, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT_PROB)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.1,\n",
        "    patience=2,\n",
        "    threshold=0.01\n",
        ")\n",
        "\n",
        "criterion_h = nn.HuberLoss()\n",
        "criterion_ce = nn.CrossEntropyLoss()\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "patience = 3\n",
        "\n",
        "print(f\"\\n--- Starting Training for {NUM_EPOCHS} Epochs ---\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    avg_loss = train_model(model, train_dataloader, optimizer, criterion_h, criterion_ce, DEVICE)\n",
        "    avg_val_loss = validate_model(model, val_dataloader, criterion_h, criterion_ce, DEVICE)\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    save_model(model.state_dict(), epoch, bucket_name)\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\n--- Early stopping triggered after {epoch+1} epochs. ---\")\n",
        "        break\n",
        "\n",
        "print(f\"Loading best model from epoch with validation loss: {best_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "XTy695fQH2bH",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50e2b3fb-e133-43f5-857b-560e8ae58ca7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training for 10 Epochs ---\n",
            "Epoch 1/10 | Batch 100% | Loss: 5.1980Epoch 1/10 -> Average Validation Loss: 5.4603\n",
            "Model saved locally to 'model/model_epoch_1.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_1.pth\n",
            "Copying file://model/model_epoch_1.pth to gs://bernhackt/models/model_epoch_1.pth\n",
            "\n",
            "Average throughput: 12.8MiB/s\n",
            "Upload complete.\n",
            "Epoch 2/10 | Batch 100% | Loss: 4.7094Epoch 2/10 -> Average Validation Loss: 5.2650\n",
            "Model saved locally to 'model/model_epoch_2.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_2.pth\n",
            "Copying file://model/model_epoch_2.pth to gs://bernhackt/models/model_epoch_2.pth\n",
            "\n",
            "Average throughput: 12.5MiB/s\n",
            "Upload complete.\n",
            "Epoch 3/10 | Batch 100% | Loss: 4.4337Epoch 3/10 -> Average Validation Loss: 5.1428\n",
            "Model saved locally to 'model/model_epoch_3.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_3.pth\n",
            "Copying file://model/model_epoch_3.pth to gs://bernhackt/models/model_epoch_3.pth\n",
            "\n",
            "Average throughput: 12.3MiB/s\n",
            "Upload complete.\n",
            "Epoch 4/10 | Batch 100% | Loss: 4.5572Epoch 4/10 -> Average Validation Loss: 5.1045\n",
            "Model saved locally to 'model/model_epoch_4.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_4.pth\n",
            "Copying file://model/model_epoch_4.pth to gs://bernhackt/models/model_epoch_4.pth\n",
            "\n",
            "Average throughput: 13.1MiB/s\n",
            "Upload complete.\n",
            "Epoch 5/10 | Batch 100% | Loss: 4.8539Epoch 5/10 -> Average Validation Loss: 5.0655\n",
            "Model saved locally to 'model/model_epoch_5.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_5.pth\n",
            "Copying file://model/model_epoch_5.pth to gs://bernhackt/models/model_epoch_5.pth\n",
            "\n",
            "Average throughput: 12.6MiB/s\n",
            "Upload complete.\n",
            "Epoch 6/10 | Batch 100% | Loss: 4.7814Epoch 6/10 -> Average Validation Loss: 5.0657\n",
            "Model saved locally to 'model/model_epoch_6.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_6.pth\n",
            "Copying file://model/model_epoch_6.pth to gs://bernhackt/models/model_epoch_6.pth\n",
            "\n",
            "Average throughput: 12.4MiB/s\n",
            "Upload complete.\n",
            "Epoch 7/10 | Batch 100% | Loss: 4.3403Epoch 7/10 -> Average Validation Loss: 5.0480\n",
            "Model saved locally to 'model/model_epoch_7.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_7.pth\n",
            "Copying file://model/model_epoch_7.pth to gs://bernhackt/models/model_epoch_7.pth\n",
            "\n",
            "Average throughput: 12.6MiB/s\n",
            "Upload complete.\n",
            "Epoch 8/10 | Batch 100% | Loss: 4.4687Epoch 8/10 -> Average Validation Loss: 5.0486\n",
            "Model saved locally to 'model/model_epoch_8.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_8.pth\n",
            "Copying file://model/model_epoch_8.pth to gs://bernhackt/models/model_epoch_8.pth\n",
            "\n",
            "Average throughput: 12.6MiB/s\n",
            "Upload complete.\n",
            "Epoch 9/10 | Batch 100% | Loss: 4.4777Epoch 9/10 -> Average Validation Loss: 5.0093\n",
            "Model saved locally to 'model/model_epoch_9.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_9.pth\n",
            "Copying file://model/model_epoch_9.pth to gs://bernhackt/models/model_epoch_9.pth\n",
            "\n",
            "Average throughput: 13.5MiB/s\n",
            "Upload complete.\n",
            "Epoch 10/10 | Batch 100% | Loss: 4.5283Epoch 10/10 -> Average Validation Loss: 4.9972\n",
            "Model saved locally to 'model/model_epoch_10.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_10.pth\n",
            "Copying file://model/model_epoch_10.pth to gs://bernhackt/models/model_epoch_10.pth\n",
            "\n",
            "Average throughput: 12.6MiB/s\n",
            "Upload complete.\n",
            "Loading best model from epoch with validation loss: 4.9972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS, 2 * NUM_EPOCHS):\n",
        "    avg_loss = train_model(model, train_dataloader, optimizer, criterion_h, criterion_ce, DEVICE)\n",
        "    avg_val_loss = validate_model(model, val_dataloader, criterion_h, criterion_ce, DEVICE)\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} -> Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    save_model(model.state_dict(), epoch, bucket_name)\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f\"\\n--- Early stopping triggered after {epoch+1} epochs. ---\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "9f7mlh5DSrf4",
        "outputId": "5f9e6c87-1272-4f9d-dc03-397193d524f8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/10 | Batch 100% | Loss: 1.9467Epoch 11/10 -> Average Validation Loss: 2.1270\n",
            "Model saved locally to 'model/model_epoch_11.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_11.pth\n",
            "Copying file://model/model_epoch_11.pth to gs://bernhackt/models/model_epoch_11.pth\n",
            "\n",
            "Average throughput: 12.5MiB/s\n",
            "Upload complete.\n",
            "Epoch 12/10 | Batch 100% | Loss: 1.9315Epoch 12/10 -> Average Validation Loss: 2.1234\n",
            "Model saved locally to 'model/model_epoch_12.pth'\n",
            "Uploading model to GCS: gs://bernhackt/models/model_epoch_12.pth\n",
            "Copying file://model/model_epoch_12.pth to gs://bernhackt/models/model_epoch_12.pth\n",
            "\n",
            "Average throughput: 13.2MiB/s\n",
            "Upload complete.\n",
            "Epoch 13/10 | Batch 35% | Loss: 1.9147"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2970802034.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_ce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mavg_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_ce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3462283740.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, criterion_h, criterion_ce, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\rEpoch {epoch+1}/{NUM_EPOCHS} | Batch {int((i+1)/len(dataloader)*100)}% | Loss: {total_loss.item():.4f}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "cQEYQRRTqK4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sequence(sequence_np, scaler, idx2cat, idx2merch, is_target=False):\n",
        "    print(\"\\n--- INPUT SEQUENCE (CONTEXT) ---\" if not is_target else \"\\n--- ACTUAL TRANSACTIONS (FOR COMPARISON) ---\")\n",
        "    num_numerical_features = len(scaler.scale_)\n",
        "\n",
        "    for i, step in enumerate(sequence_np):\n",
        "        if not is_target:\n",
        "            numerical_features_normalized = step[[0, 1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]]\n",
        "            unscaled_numerical = scaler.inverse_transform(numerical_features_normalized.reshape(1, -1))[0]\n",
        "            real_amount = unscaled_numerical[0]\n",
        "            time_delta = unscaled_numerical[2]\n",
        "            cat_id_norm = int(round(step[2]))\n",
        "            merch_id_norm = int(round(step[3]))\n",
        "        else:\n",
        "            dummy_scaled_features = np.zeros((1, num_numerical_features))\n",
        "            amount_index_in_scaler = 0\n",
        "            time_delta_index_in_scaler = 2\n",
        "\n",
        "            dummy_scaled_features[0, amount_index_in_scaler] = step[0]\n",
        "            dummy_scaled_features[0, time_delta_index_in_scaler] = step[3]\n",
        "\n",
        "            unscaled_features = scaler.inverse_transform(dummy_scaled_features)[0]\n",
        "            real_amount = unscaled_features[amount_index_in_scaler]\n",
        "            time_delta = unscaled_features[time_delta_index_in_scaler]\n",
        "\n",
        "            cat_id_norm = int(round(step[1]))\n",
        "            merch_id_norm = int(round(step[2]))\n",
        "\n",
        "\n",
        "        category_name = idx2cat.get(cat_id_norm, \"Unknown\")\n",
        "        merchant_name = idx2merch.get(merch_id_norm, \"Unknown\")\n",
        "\n",
        "        print(f\"Step {i+1:02d}: Amount: ${real_amount:10.2f} | Category: {category_name:<25} | Merchant: {merchant_name} | Time Delta: {time_delta:.2f}\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "-joNbAJiqHWD"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_transactions(input_tensor, model, scaler, idx2cat, idx2merch, device, forecast_horizon):\n",
        "    amount_preds, category_preds, merchant_preds = model.predict(input_tensor, forecast_horizon)\n",
        "\n",
        "    predictions = []\n",
        "    num_numerical_features = len(scaler.scale_)\n",
        "\n",
        "    for i in range(forecast_horizon):\n",
        "        norm_amount_val = amount_preds[:, i, :].item()\n",
        "        dummy_row = np.zeros((1, num_numerical_features))\n",
        "        dummy_row[0, 0] = norm_amount_val\n",
        "        real_amount = scaler.inverse_transform(dummy_row)[0, 0]\n",
        "\n",
        "        cat_idx = torch.argmax(category_preds[:, i, :], dim=1).item()\n",
        "        category_name = idx2cat.get(cat_idx, \"Unknown\")\n",
        "        merch_idx = torch.argmax(merchant_preds[:, i, :], dim=1).item()\n",
        "        merchant_name = idx2merch.get(merch_idx, \"Unknown\")\n",
        "\n",
        "        predictions.append(\n",
        "            f\"  - Step {i+1}: Amount: ${real_amount:.2f}, Category: '{category_name}', Merchant: '{merchant_name}'\"\n",
        "        )\n",
        "    return \"\\n\".join(predictions)"
      ],
      "metadata": {
        "id": "paY_2ueCrlQe"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98529d85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d627887d-5433-4c05-f59f-04a293675211"
      },
      "source": [
        "random_val_index = random.choice(val_indices)\n",
        "\n",
        "input_sequence_np = np.load(sequences_X_path, mmap_mode='r')[random_val_index]\n",
        "target_sequence_np = np.load(sequences_y_path, mmap_mode='r')[random_val_index]\n",
        "\n",
        "input_tensor = torch.from_numpy(input_sequence_np).float().unsqueeze(0).to(DEVICE)\n",
        "\n",
        "idx2cat = {i: name for i, name in enumerate(vocab_mappings['categories'])}\n",
        "idx2merch = {i: name for i, name in enumerate(vocab_mappings['merchants'])}\n",
        "\n",
        "display_sequence(input_sequence_np, scaler, idx2cat, idx2merch, is_target=False)\n",
        "\n",
        "print(f\"\\n--- PREDICTED TRANSACTIONS (FORECAST HORIZON: {FORECAST_HORIZON}) ---\")\n",
        "predicted_transactions = predict_next_transactions(\n",
        "    input_tensor,\n",
        "    model,\n",
        "    scaler,\n",
        "    idx2cat,\n",
        "    idx2merch,\n",
        "    DEVICE,\n",
        "    FORECAST_HORIZON\n",
        ")\n",
        "print(predicted_transactions)\n",
        "print(\"-\" * 70)\n",
        "\n",
        "display_sequence(target_sequence_np, scaler, idx2cat, idx2merch, is_target=True)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- INPUT SEQUENCE (CONTEXT) ---\n",
            "Step 01: Amount: $   -250.00 | Category: HEALTHCARE_GENERAL        | Merchant: CSS | Time Delta: 2.00\n",
            "Step 02: Amount: $   -124.45 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop | Time Delta: 1.00\n",
            "Step 03: Amount: $    -62.99 | Category: P2P_TRANSFER              | Merchant: Twint Split Bill | Time Delta: -0.00\n",
            "Step 04: Amount: $   5320.99 | Category: INCOME_GENERAL            | Merchant: Local Bank Salary | Time Delta: 3.00\n",
            "Step 05: Amount: $     -9.75 | Category: BANK_FEE                  | Merchant: Local Bank Bank Fee | Time Delta: 1.00\n",
            "Step 06: Amount: $   -127.81 | Category: INSURANCE                 | Merchant: Swica Insurance | Time Delta: -0.00\n",
            "Step 07: Amount: $   -160.00 | Category: SHOPPING_JEWELRY          | Merchant: Swiss Company | Time Delta: -0.00\n",
            "Step 08: Amount: $    -28.35 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Volg | Time Delta: 1.00\n",
            "Step 09: Amount: $    -72.90 | Category: SHOPPING_BOOKS            | Merchant: Online Bookstore | Time Delta: -0.00\n",
            "Step 10: Amount: $    -40.00 | Category: ENTERTAINMENT_STREAMING   | Merchant: YouTube Premium | Time Delta: 1.00\n",
            "Step 11: Amount: $    -90.00 | Category: ENTERTAINMENT_MOBILE      | Merchant: Local Bank Mobile | Time Delta: -0.00\n",
            "Step 12: Amount: $   -113.16 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Migros | Time Delta: 2.00\n",
            "Step 13: Amount: $   -473.61 | Category: HEALTH_INSURANCE          | Merchant: Swica | Time Delta: -0.00\n",
            "Step 14: Amount: $   -197.50 | Category: TRANSPORTATION_PUBLIC     | Merchant: Tram Monthly Pass | Time Delta: 1.00\n",
            "Step 15: Amount: $   -100.00 | Category: CASH_WITHDRAWAL           | Merchant: Raiffeisen ATM | Time Delta: -0.00\n",
            "Step 16: Amount: $  -1347.50 | Category: HOUSING_GENERAL           | Merchant: Livit AG | Time Delta: 1.00\n",
            "Step 17: Amount: $   -146.40 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop | Time Delta: 3.00\n",
            "Step 18: Amount: $    -21.96 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop Pronto | Time Delta: 3.00\n",
            "Step 19: Amount: $    -30.83 | Category: P2P_TRANSFER              | Merchant: Twint Drinks | Time Delta: 1.00\n",
            "Step 20: Amount: $   -240.50 | Category: SHOPPING_JEWELRY          | Merchant: Swiss Company | Time Delta: 1.00\n",
            "Step 21: Amount: $   -780.00 | Category: SHOPPING_ELECTRONICS      | Merchant: Digitec | Time Delta: 1.00\n",
            "Step 22: Amount: $    -42.45 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop Pronto | Time Delta: 3.00\n",
            "Step 23: Amount: $   -176.90 | Category: UTILITIES_GENERAL         | Merchant: Swisscom | Time Delta: 2.00\n",
            "Step 24: Amount: $    -50.00 | Category: FOOD_DINING_DINNER        | Merchant: Restaurant zum Kropf | Time Delta: 2.00\n",
            "Step 25: Amount: $   -207.83 | Category: SHOPPING_JEWELRY          | Merchant: Swiss Company | Time Delta: -0.00\n",
            "Step 26: Amount: $    -87.80 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Local Bank Credit Card Payment | Time Delta: -0.00\n",
            "Step 27: Amount: $    -39.50 | Category: SHOPPING_BOOKS            | Merchant: Online Bookstore | Time Delta: 1.00\n",
            "Step 28: Amount: $   -509.99 | Category: SHOPPING_JEWELRY          | Merchant: Swiss Company | Time Delta: 1.00\n",
            "Step 29: Amount: $   -110.00 | Category: SHOPPING_CLOTHING         | Merchant: Mango | Time Delta: 1.00\n",
            "Step 30: Amount: $    -43.10 | Category: P2P_TRANSFER              | Merchant: Twint Coffee | Time Delta: -0.00\n",
            "Step 31: Amount: $    -39.65 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop Pronto | Time Delta: 1.00\n",
            "Step 32: Amount: $    -46.19 | Category: P2P_TRANSFER              | Merchant: Twint Lunch | Time Delta: -0.00\n",
            "Step 33: Amount: $   -100.00 | Category: CASH_WITHDRAWAL           | Merchant: ZKB ATM | Time Delta: 1.00\n",
            "Step 34: Amount: $     -6.84 | Category: BANK_FEE                  | Merchant: Local Bank Bank Fee | Time Delta: 1.00\n",
            "Step 35: Amount: $   -169.37 | Category: INSURANCE                 | Merchant: Swica Insurance | Time Delta: -0.00\n",
            "Step 36: Amount: $   -100.00 | Category: SHOPPING_BOOKS            | Merchant: Online Bookstore | Time Delta: -0.00\n",
            "Step 37: Amount: $   6181.99 | Category: INCOME_GENERAL            | Merchant: Local Bank Salary | Time Delta: 1.00\n",
            "Step 38: Amount: $    -35.35 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Volg | Time Delta: -0.00\n",
            "Step 39: Amount: $    -40.90 | Category: SHOPPING_BOOKS            | Merchant: Online Bookstore | Time Delta: -0.00\n",
            "Step 40: Amount: $    -38.98 | Category: P2P_TRANSFER              | Merchant: Twint Drinks | Time Delta: -0.00\n",
            "Step 41: Amount: $   -192.68 | Category: ELECTRONICS_SPLURGE       | Merchant: Swiss Company | Time Delta: 1.00\n",
            "Step 42: Amount: $    -46.30 | Category: P2P_TRANSFER              | Merchant: Twint Rent Share | Time Delta: -0.00\n",
            "Step 43: Amount: $    -62.37 | Category: P2P_TRANSFER              | Merchant: Twint Drinks | Time Delta: 1.00\n",
            "Step 44: Amount: $    -48.63 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop Pronto | Time Delta: 1.00\n",
            "Step 45: Amount: $    -30.00 | Category: FOOD_DINING_LUNCH         | Merchant: Starbucks | Time Delta: -0.00\n",
            "Step 46: Amount: $    -17.56 | Category: ENTERTAINMENT_STREAMING   | Merchant: YouTube Premium | Time Delta: -0.00\n",
            "Step 47: Amount: $     -9.99 | Category: ENTERTAINMENT_MOBILE      | Merchant: Local Bank Mobile | Time Delta: -0.00\n",
            "Step 48: Amount: $   -469.83 | Category: HEALTH_INSURANCE          | Merchant: Swica | Time Delta: 2.00\n",
            "Step 49: Amount: $    -24.75 | Category: TRANSPORTATION_PUBLIC     | Merchant: Tram Monthly Pass | Time Delta: 1.00\n",
            "Step 50: Amount: $    -36.00 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Volg | Time Delta: -0.00\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "--- PREDICTED TRANSACTIONS (FORECAST HORIZON: 10) ---\n",
            "  - Step 1: Amount: $-21.18, Category: 'GENERAL_EXPENSES_DAILY', Merchant: 'Volg'\n",
            "  - Step 2: Amount: $-854.46, Category: 'HOUSING_GENERAL', Merchant: 'Livit AG'\n",
            "  - Step 3: Amount: $-235.49, Category: 'P2P_TRANSFER', Merchant: 'Swiss Company'\n",
            "  - Step 4: Amount: $-185.40, Category: 'P2P_TRANSFER', Merchant: 'Mango'\n",
            "  - Step 5: Amount: $-190.89, Category: 'P2P_TRANSFER', Merchant: 'Coop Pronto'\n",
            "  - Step 6: Amount: $-201.48, Category: 'P2P_TRANSFER', Merchant: 'Coop Pronto'\n",
            "  - Step 7: Amount: $-209.93, Category: 'P2P_TRANSFER', Merchant: 'Coop Pronto'\n",
            "  - Step 8: Amount: $-215.82, Category: 'P2P_TRANSFER', Merchant: 'Coop Pronto'\n",
            "  - Step 9: Amount: $-222.12, Category: 'P2P_TRANSFER', Merchant: 'Coop Pronto'\n",
            "  - Step 10: Amount: $-225.91, Category: 'GENERAL_EXPENSES_DAILY', Merchant: 'Coop Pronto'\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "--- ACTUAL TRANSACTIONS (FOR COMPARISON) ---\n",
            "Step 01: Amount: $    -10.00 | Category: SHOPPING_JEWELRY          | Merchant: Swiss Company | Time Delta: -0.00\n",
            "Step 02: Amount: $    -94.10 | Category: GIFT_PURCHASE             | Merchant: Online Gift Store | Time Delta: -0.00\n",
            "Step 03: Amount: $  -2169.99 | Category: HOUSING_GENERAL           | Merchant: Swiss Prime Site | Time Delta: 1.00\n",
            "Step 04: Amount: $    -18.67 | Category: P2P_TRANSFER              | Merchant: Twint Drinks | Time Delta: 3.00\n",
            "Step 05: Amount: $    -29.07 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop Pronto | Time Delta: 3.00\n",
            "Step 06: Amount: $   -580.00 | Category: SHOPPING_ELECTRONICS      | Merchant: Digitec | Time Delta: -0.00\n",
            "Step 07: Amount: $    -37.64 | Category: GENERAL_EXPENSES_DAILY    | Merchant: Coop Pronto | Time Delta: 3.00\n",
            "Step 08: Amount: $    -11.90 | Category: FOOD_DINING_LUNCH         | Merchant: Coop Restaurant | Time Delta: 2.00\n",
            "Step 09: Amount: $   -100.00 | Category: CASH_WITHDRAWAL           | Merchant: ZKB ATM | Time Delta: -0.00\n",
            "Step 10: Amount: $    -73.07 | Category: GIFT_PURCHASE             | Merchant: Online Gift Store | Time Delta: -0.00\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}